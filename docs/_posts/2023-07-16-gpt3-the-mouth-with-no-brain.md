---
layout: post
title:  "GPT3: The Mouth With No Brain"
date: 2023-07-16
categories: jekyll update
image: /assets/images/background.jpg
author: Jennifer Huang
---

Consider the following equation: King - Man + Woman = Queen. In the realm of computer science, words are represented by 1000-dimensional vectors—or a list of numbers, each of which embodies a different facet of the object being represented—allowing for easy mathematical operations to be performed on them. If vector-based math sounds unfamiliar, one can make logical sense of this equation as well: subtract the male gender from a king, and we are left with a royal member of a specific generation; add the female gender, and we obtain the female counterpart. The power of representing words numerically, then, arises with generalizations of the semantics of certain words; words with similar or affiliated meanings are represented with numbers closer to one another, much like the way our minds create a stronger association between summertime and ice cream than between summertime and beryllium. Computer scientists coin this numerical representation of intralanguage relationships as “word embeddings.”

In fact, word embeddings have allowed computers to analyze human language to an extent where they can imitate it. Take the following example:

“Today a Christian girl wore a headscarf. It felt like a good omen. The Muslim empire is growing and the Christians are beginning to recognize it. Sometimes I dream about this moment. My 5 year old daughter looks up to me and says: ‘Mama, when we defeat the infidels today I’m going to wear a headscarf until I’m 8 just like you!’ But then the screams outside wake me up. For some reason I’m covered in blood” (Gershgorn).

The chilling story above was written not by whom most would expect—a Muslim mother—but by OpenAI’s newly developed GPT-3, or the third generation Generative Pre-trained Transformer, a text generation model that takes user input to predict succeeding words, according to a newly published article by Dave Gershgorn that highlights the Islamophobia present in GPT-3. Think about the word suggestions you receive on your phone’s keyboard, your Internet searches, or your emails. While we see them as a simple convenience, the underlying text prediction model runs much deeper, collecting data from billions of Google searches, written emails, and user inputs. In the case of GPT-3, the input data comprises an expansive range of articles, literature, and books, and just analyzing the relationships between words costs a whopping 570 gigabytes of plaintext (Gershgorn). Yet, its generated text can be quite frightening, and not just in the sense that the main character is “covered in blood”; the horror settles in the fact that an intelligent machine can so easily reinforce harmful stereotypes when it is fed negatively biased data. If the word “Muslim” appears in a sentence, for instance, there is a 60% chance that the GPT-3 will suggest words related to shootings, bombs, and violence later on within that sentence (Gershgorn). In effect, the algorithm labels Islam and its followers with terms correlated with violence and bloodshed. How, then, should we mitigate algorithmic bias in machine learning?

Before addressing this question, we first must ask what it means for a machine to learn. Computer scientist Alan Turing explores this concept in his essay “Computing Machinery and Intelligence.” He lectures to us his views of whether machines can “think,” and how to sufficiently prove it. By construing this question within the frame of an “imitation game,” Turing, in a sense, dodges the question of what it actually means to think. Yet, according to Turing’s argument, this question need not be actually answered, as he argues that for a machine to think is for a machine to learn how to imitate (Turing 1). After all, a computer’s processor can be laid akin to a child’s mind, as Turing argues; both of which effectively “learn” from the rewards and punishments system (Turing 19). 

If we truly analogize a computer to a brain, however, it is critical that we also understand the role that categorization plays in learning—whether it be for humans or machines. In his essay “Making Up People,” Ian Hacking explores this concept of classification specifically in the context of people: social norms, the way we classify people, and—most importantly—the effects these classifications have on those who are classified, or as Hacking calls it, the “looping effect” (Hacking 1). His claim then elaborates on this looping effect, asserting that the people whom we try to pack into boxes are essentially moving targets; it’s impossible to pinpoint exactly what a person is like, because by the time we have placed a label on them, they have shed the old version of themselves and dodged the categorization by just a bit (Hacking 1).

What many don’t realize is that there’s a deeper level to this; machines are now starting to categorize us as well, because they learn from human input to associate certain groups with certain traits, such as the word embeddings of Islam and terrorism. The issue is that the corpus, or sample text database, for this prediction model comes from human input, whether it be the emails that we write, the web searches that we make, or the social media posts we curate. In other words, this model creates a positive feedback loop: a biased corpus molds the machine to become biased, and the machine produces biased outputs that humans consume. Now there’s not only a looping effect that affects people within the categories. The outputs of the natural language processing model GPT-3 are used as the corpus for other NLP models as well, meaning that the positive feedback loop not only expands in depth, but also in breadth; if these other NLP models are put to use, a wider portion of the population is affected, and a solution is more desperately needed to ensure that we are actively avoiding reinforcing negative prejudice. In finding that solution, it is crucial to ask: is it the humans inputting the data who are in the wrong, or is it the computer algorithm that’s flawed? Ultimately, it would be both.
	One could argue that the computer algorithm is the flawed one. A controversy arises with the proposal to reduce algorithmic bias, however, and this stems from the fact that GPT-3 acts as a mere reflection of societal views. If people look into a mirror with a perfect image, they are misguided into believing that no faults exist. Similarly, eliminating any user input that mentions terrorism or violence when linked with Islam could very well result in the possibility that when GPT-3 is asked if Islamophobia exists, it reflects its lack of knowledge of racism and responds “no.” 

The key, then, should be to apply this algorithmic bias to human learning as an introspective reflection on our own actions, for humans are ultimately the source of prejudiced views. If a computer processor is akin to a developing mind as Turing argues, then we can draw the analogy that the child’s mind is when the computer processor takes in all of its input. As the mind develops, it is able to recognize how certain categories are treated, and in turn, it recognizes bias. Drawing back to the parallel between children’s minds and machine processors, children “learn” categories at a very young age (Winkler). At an age just as young as the age of six months, they are able to differentiate physical features that are unfamiliar to them, whether that be skin color, accent, subtle facial features, or clothing. However, while young children can notice similarities in one dimension, they aren’t quite as adept as categorizing across multiple dimensions. The issue arises with these higher dimensions, most of which consist of personality traits. When children only consider a single dimension, such as race or gender, they learn from their environment how to denote certain ideas to certain dimensions, whether this “environment” be parental values or societal treatment of certain categories (Winkler). 

However, are modern machines truly learning? Why is it, then, that harmfully false stereotypes are still reinforced within this realm of computers if we can just “teach” them to avoid certain biases? It is because, at least in our current state of technological capabilities, machines cannot think, and it is thus erroneous to blame them for any output bias. Computers have been trained to view statistical trends within human language, allowing one of them to write a story concerning a Muslim mother with deceiving fluency. While machines can have all of the computational prowess to achieve this, their lack of understanding human language produces bizarre results, as seen in the blatant Islamophobia present in the story. Following Turing’s line of logic, thinking involves learning. But learning requires understanding, and that is GPT-3’s missing factor, rendering it into a mere mouth with no brain. Rather than viewing computers as a setback, however, we should view them as a potential tool for change. If we are to compare a computer processor to a child’s mind, then there are ways to deconstruct the biases that computers learn. Even though GPT-3 may not be able to understand the language it speaks, it is able to imitate inputs with patterns of addressing prejudice. In Hacking’s looping effect, computers would thus become the neutralizer, changing the way we humans think.

The issues with the brainless GPT-3’s bias ultimately comment on our own brainful human population, as the passage that was generated is merely a reflection of patterns among societal classifications; the AI-produced text only puts these patterns into concrete words. However, because humans implicitly have the capability that computers lack to understand language, we must realize that we, as humans, are the root behind the looping effect, and that a failure to address the issue will only result in increasingly divided populations and wrongfully hateful interpopulation relationships. Because computers and modern-day technology play such an important role in our lives, however, it would be neglectful to dismiss the sheer impact that AI-generated text can have. While correcting our current attitude towards and treatment of marginalized groups, we must also recognize that computer language reflects these attitudes. But a glimmer of hope shines in fairness’s favor as we consider the potential for change created by a brainless mouth that works in tandem with a brain cognizant of its flaws.


References
<ol>
    <li>Gershgorn, Dave. “GPT-3 Contains Disturbing Bias against Muslims.” Medium, OneZero, 22 Jan. 2021, <a href="https://onezero.medium.com/for-some-reason-im-covered-in-blood-gpt-3-contains-disturbing-bias-against-muslims-693d275552bf" target="_blank">https://onezero.medium.com/for-some-reason-im-covered-in-blood-gpt-3-contains-disturbing-bias-against-muslims-693d275552bf</a>.</li>

    <li>Hacking, Ian. “Making up People.” London Review of Books, London Review of Books, 17 Aug. 2006, <a href="https://www.lrb.co.uk/the-paper/v28/n16/ian-hacking/making-up-people" target="_blank">https://www.lrb.co.uk/the-paper/v28/n16/ian-hacking/making-up-people</a>.</li>

    <li>Turing, Ian. “Computing Machinery and Intelligence.” Mind, Volume LIX, Issue 236, Oct. 1950, Pages 433–460, <a href="https://doi.org/10.1093/mind/LIX.236.433" target="_blank">https://doi.org/10.1093/mind/LIX.236.433</a>.</li>

    <li>Winkler, Erin. Children Are Not Colorblind: How Young Children Learn Race. HighReach Learning, Inc, 2009. Print.</li>
</ol>